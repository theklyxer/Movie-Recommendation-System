{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd     #importing required libraries\n",
    "import chardet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "credits = pd.read_csv('credits.csv')       #loading credits into dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "movies = pd.read_csv('movies_metadata.csv')  #loading movies into dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "movies.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "credits.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "selected_columns = ['cast', 'crew','movieid']\n",
    "credits = credits[selected_columns]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "credits.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "imp = pd.read_csv('impwords.csv')  #loading movies into dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "imp.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "movies.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "movies.isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "movies.dropna(inplace=True) #now we are dropping the empty cells in which we have no use "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "movies.isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "movies.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "credits.dropna(inplace=True) #now we are dropping the empty cells in which we have no use "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "credits.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "imp.dropna(inplace=True) #now we are dropping the empty cells in which we have no use "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "imp.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "movies.isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "imp.isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "credits.isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Assuming 'movieid' column in both DataFrames has mismatched types\n",
    "imp['movieid'] = imp['movieid'].astype(str)  # Convert to 'object' type (string)\n",
    "\n",
    "# Now perform the merge\n",
    "movies = movies.merge(imp, on='movieid')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "movies.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "movies.isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "credits.isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "credits.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "movies.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "merged_df = pd.merge(credits, movies, on='movieid')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "merged_df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "credits = credits.dropna(subset=['movieid'])\n",
    "movies = movies.dropna(subset=['movieid'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "credits['movieid'] = credits['movieid'].astype(str).str.strip()\n",
    "movies['movieid'] = movies['movieid'].astype(str).str.strip()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "up_df = pd.merge(credits, movies, on='movieid')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "up_df.shape           #remember up_df is the main data frame upto now"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "up_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import ast"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def conv(var):\n",
    "    return [i['name'] for i in ast.literal_eval(var)]          #we are converting the given data into list "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "up_df['genres'] = up_df['genres'].apply(conv)\n",
    "up_df.head()                                       #adding converted data to generes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "up_df['keywords'] = up_df['keywords'].apply(conv)\n",
    "up_df.head()                                       #adding converted data to generes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fetchthedirector(var):\n",
    "    L = []\n",
    "    for i in ast.literal_eval(var):\n",
    "        if i['job'] == 'Director':\n",
    "            L.append(i['name'])\n",
    "    return L "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "up_df['crew'] = up_df['crew'].apply(fetchthedirector)\n",
    "up_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "up_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "  # Import the ast module for literal_eval\n",
    "\n",
    "# Assuming up_df is your existing DataFrame with a 'cast' column\n",
    "\n",
    "# Convert string representations to dictionaries using ast.literal_eval\n",
    "up_df['cast'] = up_df['cast'].apply(ast.literal_eval)\n",
    "\n",
    "# Function to extract the top 3 cast members for a given row\n",
    "def extract_top3_cast(row):\n",
    "    # Sorting the cast list based on the 'order' key\n",
    "    sorted_cast = sorted(row, key=lambda x: x.get('order', 0))\n",
    "    # Extracting the information for the top 3 cast members\n",
    "    top3_cast_info = sorted_cast[:3]\n",
    "    return top3_cast_info\n",
    "\n",
    "# Applying the function to the 'cast' column to create a new column 'top3_cast'\n",
    "up_df['top3_cast'] = up_df['cast'].apply(extract_top3_cast)\n",
    "\n",
    "# Displaying the resulting DataFrame\n",
    "print(up_df[['cast', 'top3_cast']])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "up_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def conve4(var):\n",
    "    L = []\n",
    "    counter = 0 \n",
    "    \n",
    "    for i in var:\n",
    "        if counter < 3:\n",
    "            L.append(i['name'])\n",
    "            counter += 1\n",
    "            \n",
    "    return L\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    " up_df['top3_cast'] = up_df['top3_cast'].apply(conve4)\n",
    " up_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "up_df.drop(columns=['cast'], inplace=True)\n",
    "up_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "up_df['overview'] = up_df['overview'].apply(lambda x:x.split())\n",
    " #converting overview into list using lamda function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "up_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "up_df['tags'] = up_df['overview'] + up_df['genres'] + up_df['keywords'] + up_df['top3_cast'] + up_df['crew']#merging them into one "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "up_df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "up_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "up_df['tags'] = up_df['tags'].apply(lambda x: \" \".join(x))\n",
    "up_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_df = up_df[[  'movieid', 'title',  'tags']]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_df['tags'] = new_df['tags'].apply(lambda x:x.lower())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_df['title'] = new_df['title'].apply(lambda x:x.lower())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_df.shape\n",
    "new_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.tokenize import word_tokenize\n",
    "nltk.download('punkt')\n",
    "nltk.download('wordnet')\n",
    "\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "def lemmatize(text):\n",
    "    words = word_tokenize(text)\n",
    "    lemmatized_words = [lemmatizer.lemmatize(word) for word in words]\n",
    "    return \" \".join(lemmatized_words)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_df['tags'] = new_df['tags'].apply(lemmatize)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "def remove_noise(text):\n",
    "    # Remove HTML tags\n",
    "    text = re.sub(r'<.*?>', '', text)\n",
    "    \n",
    "    # Remove punctuation and other non-alphabetic characters\n",
    "    text = re.sub(r'[^a-zA-Z\\s]', '', text)\n",
    "    \n",
    "    return text\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_df['tags'] = new_df['tags'].apply(remove_noise)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "nltk.download('stopwords')\n",
    "nltk.download('punkt')\n",
    "\n",
    "def remove_stopwords(text):\n",
    "    stop_words = set(stopwords.words('english'))\n",
    "    words = word_tokenize(text)\n",
    "    filtered_words = [word for word in words if word.lower() not in stop_words]\n",
    "    return \" \".join(filtered_words)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_df['tags'] = new_df['tags'].apply(remove_stopwords)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_df['tags'] = new_df['tags'].apply(lambda x: ' '.join([word for word in x.split() if len(word) > 2]))\n",
    "#Removing Short Words:\n",
    "\n",
    "#Eliminate very short words (e.g., words with one or two characters) as they might not contribute much meaning and can introduce noise."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.tokenize import word_tokenize\n",
    "new_df['tags'] = new_df['tags'].apply(word_tokenize)\n",
    "#Ensure that your text is tokenized into individual words. This is essential for most natural language processing tasks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.corpus import wordnet\n",
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "def replace_negations(words):\n",
    "    for i in range(len(words)):\n",
    "        if words[i] == 'not' and i + 1 < len(words):\n",
    "            antonym = wordnet.morphy(words[i + 1], wordnet.ADJ)\n",
    "            if antonym:\n",
    "                words[i + 1] = antonym\n",
    "    return words\n",
    "\n",
    "# Apply the function to each row of the 'tags' column\n",
    "new_df['tags'] = new_df['tags'].apply(lambda x: replace_negations(x) if isinstance(x, list) else x)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from rank_bm25 import BM25Okapi\n",
    "\n",
    "\n",
    "# Initialize BM25 model\n",
    "bm25 = BM25Okapi(new_df['tags'])\n",
    "\n",
    "# Function to get top N similar movie titles for a given movie title\n",
    "def get_top_similar_movie_titles(movie_title, n=5):\n",
    "    movie_content = new_df.loc[new_df['title'] == movie_title, 'tags'].iloc[0]\n",
    "    scores = bm25.get_scores(movie_content)\n",
    "    top_indices = sorted(range(len(scores)), key=lambda i: scores[i], reverse=True)[1:n+1]  # Exclude the movie itself\n",
    "    return new_df.iloc[top_indices]['title'].tolist()\n",
    "\n",
    "# Example usage\n",
    "movie_title = \"toy story\"\n",
    "top_similar_movie_titles = get_top_similar_movie_titles(movie_title)\n",
    "for i, title in enumerate(top_similar_movie_titles, start=1):\n",
    "    print(f\"{i}. {title}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pip install tensorflow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "import re\n",
    "import numpy as np\n",
    "from sentence_transformers import SentenceTransformer\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "import pandas as pd\n",
    "\n",
    "class MovieRecommender:\n",
    "    def __init__(self, df=None, title_column=None, tags_column=None):\n",
    "        self.df = df\n",
    "        self.title_column = title_column\n",
    "        self.tags_column = tags_column\n",
    "\n",
    "    def process(self):\n",
    "        # Assuming the 'tags' column contains lists, convert them to strings\n",
    "        self.df[self.tags_column] = self.df[self.tags_column].apply(lambda x: ' '.join(x))\n",
    "        # Combine 'title' and 'tags' into a single column for corpus\n",
    "        self.df['corpus'] = self.df[self.title_column] + '. ' + self.df[self.tags_column]\n",
    "\n",
    "    def get_bert_weights(self, corpus):\n",
    "        model = SentenceTransformer('bert-base-nli-mean-tokens')\n",
    "        vectors = model.encode(corpus)\n",
    "        similarity_matrix = cosine_similarity(vectors)\n",
    "        weights = pd.DataFrame(similarity_matrix)\n",
    "        return weights\n",
    "\n",
    "def recommendation(self, movie_title, weights, n):\n",
    "        movie_index = self.df[self.df[self.title_column] == movie_title].index[0]\n",
    "        similarities = weights.iloc[movie_index].values\n",
    "        similar_indices = np.argsort(-similarities)[1:n + 1]\n",
    "        similar_movies = self.df.loc[similar_indices, self.title_column].values\n",
    "        return similar_movies\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mr = MovieRecommender(df=new_df, title_column='title', tags_column='tags')\n",
    "mr.process()  # Process the data\n",
    "corpus = mr.df['corpus'].values  # Get corpus directly from DataFrame\n",
    "wts_df = mr.get_bert_weights(corpus)  # Get BERT weights\n",
    "wts_df.head()  # Display the head of the DataFrame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "movies_recommended = mr.recommendation('toy story', wts_df, 5)\n",
    "print(movies_recommended)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
